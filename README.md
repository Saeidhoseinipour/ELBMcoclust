[![License: MIT](https://img.shields.io/badge/License-MIT-orange.svg)](LICENSE)
[![](https://badgen.net/badge/DOI/10.1007-s11634-024-00608-3/orange?icon=instgrame)](https://link.springer.com/article/10.1007/s11634-024-00608-3#rightslink)
[![https://github.com/Saeidhoseinipour/NMTFcoclust](https://badgen.net/badge/Purchase/PDF/orange?icon=instgrame)](https://order.springer.com/public/cart?message=AddToCartSuccess)
[![https://github.com/Saeidhoseinipour/NMTFcoclust](https://badgen.net/badge/ELBM/Coclust/orange?icon=instgrame)](https://github.com/Saeidhoseinipour/ELBMcoclust/blob/main/Models/coclust_ELBMcem.py)
[![https://github.com/Saeidhoseinipour/NMTFcoclust](https://badgen.net/badge/SELBM/Coclust/orange?icon=instgrame)](https://github.com/Saeidhoseinipour/ELBMcoclust/blob/main/Models/coclust_SELBMcem.py)
[![https://github.com/Saeidhoseinipour/EM-typecoclust/tree/main](https://badgen.net/badge/Supplementary/Material/orange?icon=instgrame)](https://github.com/Saeidhoseinipour/EM-typecoclust/tree/main)
[![https://github.com/Saeidhoseinipour/EM-typecoclust/tree/main](https://badgen.net/badge/Classic3/Dataset/orange?icon=instgrame)](https://github.com/Saeidhoseinipour/ELBMcoclust/tree/main/Datasets#readme)
[![](https://badgen.net/badge/Research/gate/orange?icon=instgrame)](https://www.researchgate.net/publication/385282893_A_sparse_exponential_family_latent_block_model_for_co-clustering)
[![](https://badgen.net/badge/ivy/sci/orange?icon=instgrame)](https://www.ivysci.com/articles/5816491__A_sparse_exponential_family_latent_block_model_for_coclustering)
[![](https://badgen.net/badge/ouci/gov/orange?icon=instgrame)](https://ouci.dntb.gov.ua/en/works/42rEnmW4/)
[![](https://badgen.net/badge/x/mol/orange?icon=instgrame)](https://www.x-mol.com/paper/math/tag/106/journal/19252)
[![](https://badgen.net/badge/Pee/ref/orange?icon=instgrame)](https://www.peeref.com/works/83810598)
[![](https://badgen.net/badge/centre/borelli/orange?icon=instgrame)](https://centreborelli.ens-paris-saclay.fr/fr/publications/sparse-exponential-family-latent-block-model-co-clustering)
[![](https://badgen.net/badge/HAL/science/orange?icon=instgrame)](https://mnhn.hal.science/ENS-PARIS-SACLAY/hal-04855166v1)
[![](https://badgen.net/badge/Semantic/scholar/orange?icon=instgrame)](https://www.semanticscholar.org/paper/A-sparse-exponential-family-latent-block-model-for-Hoseinipour-Aminghafari/76a9d53e0827d36eaa206e035c45d4d424894663)
[![](https://badgen.net/badge/Colab/ws/orange?icon=instgrame)](https://colab.ws/articles/10.1007%2Fs11634-024-00608-3)
[![](https://badgen.net/badge/OU/CI/orange?icon=instgrame)](https://ouci.dntb.gov.ua/en/works/42rEnmW4/)








<!--
[![https://github.com/Saeidhoseinipour/NMTFcoclust](https://badgen.net/badge/Orginal/Paper/pink?icon=instgrame)](https://github.com/Saeidhoseinipour/ELBMcoclust/blob/main/Models/coclust_SELBMcem.py)
[![https://github.com/Saeidhoseinipour/NMTFcoclust](https://badgen.net/badge/DOI:/1011.18363.32761/green?icon=instgrame)](https://github.com/Saeidhoseinipour/ELBMcoclust/blob/main/Models/coclust_SELBMcem.py)
[![Supplementary material](https://badgen.net/badge/Supplementary/material/orange?icon=instgrame)](https://ars.els-cdn.com/content/image/1-s2.0-S095741742301182X-mmc1.pdf)
[![https://github.com/Saeidhoseinipour/NMTFcoclust](https://badgen.net/badge/Purchase/PDF/brown?icon=instgrame)](https://www.sciencedirect.com/getaccess/pii/S095741742301182X/purchase)
-->



<img alt="Screenshot: 'README.md'" src="https://github.com/Saeidhoseinipour/ELBMcoclust/blob/main/Images/ADAC.png?raw=true" width="96%">

## Table of Contents

<table>
  <tr>
    <td style="vertical-align: top;">
      <ul>
        <li><a href="#elbm-coclust-and-selbm-coclust">ELBMcoclust and SELBMcoclust</a></li>
        <li><a href="#datasets">Datasets</a></li>
        <li><a href="#Implement">Implement</a></li>
        <li><a href="#visualization">Visualization</a></li>
        <li><a href="#word-cloud-of-poissonselbm-for-classic3">Word Cloud of PoissonSELBM for Classic3</a></li>
	<li><a href="#contributions">Contributions</a></li>
        <li><a href="#highlights">Highlights</a></li>
        <li><a href="#supplementary-materials">Supplementary Materials</a></li>
        <li><a href="#data-availability">Data Availability</a></li>
	<li><a href="#cite">Cite</a></li>
        <li><a href="#references">References</a></li>
      </ul>
    </td>
    <td>
      <img src="https://github.com/Saeidhoseinipour/ELBMcoclust/blob/main/Images/WC_classic3_three_color_3_3.svg" alt="A sparse exponential family latent block model for co-clustering, Saeid Hoseinipour" style="width:250px; box-shadow: 5px 5px 15px rgba(0,0,0,0.3); transform: rotateY(10deg);">
    </td>
    <td>
      <img src="https://github.com/Saeidhoseinipour/ELBMcoclust/blob/main/Images/bar_chart_words_classic3_V4_3_3.svg" alt="A sparse exponential family latent block model for co-clustering, Saeid Hoseinipour" style="width:200px; box-shadow: 5px 5px 15px rgba(0,0,0,0.3); transform: rotateY(-10deg);">
    </td>
  </tr>
</table>







<!--
<img alt="Screenshot: 'README.md'" src="https://github.com/user-attachments/assets/92f7f27c-21e1-49c2-a116-a0180bdcd02e?raw=true" width="50%">
-->





# **`ELBMcoclust`** and **`SELBMcoclust`**

Sparse and Non-Sparse Exponential Family Latent Block Model for Co-clustering

The goal of the statistical approach is to analyze the behavior of the data by considering the probability distribution. The complete log-likelihood function for three version of LBM, Exponential LBM and Sparse Exponential LBM,  will be as follows:
-  **LBM**
```math
		L^{\text{LBM}}(\mathbf{r},\mathbf{c},\boldsymbol{\gamma})= \sum\limits_{i,k}r_{ik} \log\pi_{k} +\sum\limits_{j,h}  \log\rho_{h} c^{\top}_{jh}+
		\sum\limits_{i,j,k,h} r_{ik}\log \varphi(x_{ij};\alpha_{kh})c^{\top}_{hj}.
```
-  **ELBM**
```math
\begin{align*}
	L^{\text{ELBM}}(\mathbf{r},\mathbf{c},\boldsymbol{\gamma}) \propto& 
	\sum\limits_{i,k}r_{ik} \log\pi_{k} +\sum\limits_{j,h}  \log\rho_{h} c^{\top}_{jh}  +
	\text{Tr}\left(
	(\mathbf{R}^{\top} (\mathbf{S_{x}}\odot \hat{\boldsymbol{\beta}}) \mathbf{C})^{\top}
	\mathbf{A}_{\boldsymbol{\alpha}}
	\right)\nonumber\\
	&- \text{Tr}\left(
	(\mathbf{R}^{\top} (\mathbf{E}_{mn}\odot
	\hat{\boldsymbol{\beta}}) \mathbf{C})^{\top}
	\mathbf{F}_{\boldsymbol{\alpha}}
	\right).
\end{align*}
```
-  **SELBM**
```math
\begin{align*}
	L^{\text{SELBM}}(\mathbf{r},\mathbf{c},\boldsymbol{\gamma})
	\propto&
	\sum\limits_{k} r_{.k} \log\pi_{k} +	
	\sum\limits_{h}  c_{.h}\log\rho_{h}
	+
	\sum\limits_{k} 
	\left[
	\mathbf{R}^{\top}(\mathbf{S_{x}}\odot \hat{\boldsymbol{\beta}})\mathbf{C}	
	\right]_{kk}
	\left(
	A(\alpha_{kk}) - A(\alpha)
	\right)\nonumber\\
	&-  
	\sum\limits_{k}  
	[\mathbf{R}^{\top}	(\mathbf{E}_{mn} \odot \hat{\boldsymbol{\beta}} )\mathbf{C}]_{kk} 
	\left(
	F(A(\alpha_{kk})) -F(A(\alpha)) 
	\right).
\end{align*}
```

\begin{align*}
L^{\text{SELBM}}(\mathbf{r},\mathbf{c},\boldsymbol{\gamma})
\propto{}&
\sum_{k} r_{\cdot k}\,\log\pi_{k}
+ \sum_{h} c_{\cdot h}\,\log\rho_{h}
+ \sum_{k}
\bigl[\mathbf{R}^\top\,(S_x\odot\hat{\boldsymbol\beta})\,\mathbf{C}\bigr]_{kk}\,
\bigl(A(\alpha_{kk}) - A(\alpha)\bigr)\\
&\quad
- \sum_{k}
\bigl[\mathbf{R}^\top\,(E_{mn}\odot\hat{\boldsymbol\beta})\,\mathbf{C}\bigr]_{kk}\,
\bigl(F\bigl(A(\alpha_{kk})\bigr) - F\bigl(A(\alpha)\bigr)\bigr)\,.
\end{align*}




![](https://github.com/Saeidhoseinipour/ELBMcoclust/blob/main/Images/WebACE_SELBMvsELBM.png?raw=true)

## [Datasets](https://github.com/Saeidhoseinipour/ELBMcoclust/tree/main/Datasets)


| **Datasets**    | **Topics**                                | **#Classes** | **(#Documents, #Words)** | **Sparsity(%0)** | **Balance** |
|-----------------|-------------------------------------------|--------------|-------------------------------------|-------------------|-------------|
|  [Classic3](https://github.com/Saeidhoseinipour/NMTFcoclust/blob/master/Datasets/classic3.mat)        | Medical, Information retrieval, Aeronautical systems | 3            | (3891, 4303)               | 98.95            | 0.71        |
| [CSTR](https://github.com/Saeidhoseinipour/NMTFcoclust/blob/master/Datasets/cstr.mat)            | Robotics/Vision, Systems, Natural Language Processing, Theory | 4            | (475, 1000)                | 96.60            | 0.399       |
| [WebACE](https://github.com/Saeidhoseinipour/NMTFcoclust/blob/master/Datasets/WebACE..mat)          | 20 different topics from WebACE project | 20           | (2340, 1000)               | 91.83            | 0.169       |
| [Reviews](https://github.com/Saeidhoseinipour/NMTFcoclust/blob/master/Datasets/Reviews.mat)         | Food, Music, Movies, Radio, Restaurants | 5            | (4069, 18483)              | 98.99            | 0.099       |
|  [Sports](https://github.com/Saeidhoseinipour/NMTFcoclust/blob/master/Datasets/Sports.mat)          | Baseball, Basketball, Bicycling, Boxing, Football, Golfing, Hockey | 7            | (8580, 14870)              | 99.14            | 0.036       |
| [TDT2](https://github.com/Saeidhoseinipour/NMTFcoclust/blob/master/Datasets/TDT2.mat)            | 30 different topics                     | 30           | (9394, 36771)              | 99.64            | 0.028       |

* Balance: (\#documents in the smallest class)/(\#documents in the largest class)

## [Implement](https://github.com/Saeidhoseinipour/ELBMcoclust/tree/main/Models)
```python
from ELBMcoclust.Models.coclust_ELBMcem import CoclustELBMcem
from ELBMcoclust.Models.coclust_SELBMcem import CoclustSELBMcem
```
```python
from NMTFcoclust.Evaluation.EV import Process_EV

ELBM = CoclustELBMcem(n_row_clusters = 4, n_col_clusters = 4, model = "Poisson")
ELBM.fit(X_CSTR)

SELBM = CoclustSELBMcem(n_row_clusters = 4, n_col_clusters = 4, model = "Poisson")
SELBM.fit(X_CSTR)

Process_Ev = Process_EV(true_labels ,X_CSTR, ELBM) 
```

```python
from sklearn.metrics import confusion_matrix 

confusion_matrix(true_labels, np.sort(ELBM.row_labels_))


       [[101,   0,   0,   0],
        [  4,  52,  15,   0],
        [  0,   0,  178,  0],
        [  0,   0,   34, 91]]   
```





## Visualization
-  **Confusion Matrices**
<img alt="Screenshot: 'README.md'" src="https://github.com/Saeidhoseinipour/ELBMcoclust/blob/main/Images/CM_3_dataset.png?raw=true" width="100%">

-  **Vertical Bar chart**
<img alt="A sparse exponential family latent block model for co-clustering, Text mining, Matrix factorization, Co-clustering, Saeid Hoseinipour" src="https://github.com/Saeidhoseinipour/ELBMcoclust/blob/main/Images/bar_chart_all_words_classic3_top_100.svg">

-  **Horizontal Bar chart**

<img alt="A sparse exponential family latent block model for co-clustering, Text mining, Matrix factorization, Co-clustering, Saeid Hoseinipour" src="https://github.com/Saeidhoseinipour/ELBMcoclust/blob/main/Images/bar_chart_all_words_classic3_top_1000.svg">



- **Box plots**
<img alt="A sparse exponential family latent block model for co-clustering, Screenshot: 'README.md', saeid hoseinipour" src="https://github.com/Saeidhoseinipour/ELBMcoclust/blob/main/Images/Box_plots.png?raw=true" width="100%">


- **Scatter plots**
<img alt="A sparse exponential family latent block model for co-clustering, Screenshot: 'README.md', saeid hoseinipour" src="https://github.com/Saeidhoseinipour/ELBMcoclust/blob/main/Images/Scatter_plots_2.png?raw=true" width="100%">

- **Swarm plots**
<img alt="A sparse exponential family latent block model for co-clustering, Screenshot: 'README.md', saeid hoseinipour" src="https://github.com/Saeidhoseinipour/ELBMcoclust/blob/main/Images/swarm_plot.png?raw=true" width="100%">

- **Reorganized 3*3 Word cloud of `PoissonSELBM` for Classic3**

<!-- 
<img alt="A sparse exponential family latent block model for co-clustering, Screenshot: 'README.md'" src="https://github.com/Saeidhoseinipour/ELBMcoclust/blob/main/Results/SELBM_Classic3.gif" width="45%">
 --> 

<!--
<img alt="A sparse exponential family latent block model for co-clustering, Screenshot: 'README.md'" src="https://github.com/Saeidhoseinipour/ELBMcoclust/blob/main/Images/Wordcloud_Classic3_SELBM.png?raw=true" width="100%">
-->

<!--
![WC_ELBMcoclust](https://github.com/Saeidhoseinipour/ELBMcoclust/assets/43203342/5f33f01d-2236-4a47-9cf0-cf904fd047c3)
-->
<img alt="A sparse exponential family latent block model for co-clustering, Word clouds top 60 words in classic3 dataset obtined by PoissonSELBM for co-clustering,  Latent Block Model, Text mining, Matrix factorization, Co-clustering, Saeid Hoseinipour, clustering" src="https://github.com/Saeidhoseinipour/ELBMcoclust/blob/main/Images/WC_classic3_three_color_3_3.svg">

- **Reorganized 3*3 Bar Chart of `PoissonSELBM` for Classic3**
  
<img alt="A sparse exponential family latent block model for co-clustering, Bar charts top 50 words in classic3 dataset obtined by PoissonSELBM for co-clustering, Saeid Hoseinipour, text mining, clustering, Expoential family, Latent Block Model, Text mining, Matrix factorization, Co-clustering, Saeid Hoseinipour" src="https://github.com/Saeidhoseinipour/ELBMcoclust/blob/main/Images/bar_chart_words_classic3_V4_3_3.svg">



## Contributions 

In this paper, we provide a summary of the main contributions:

- **Exponential family Latent Block Model (ELBM) and Sparse version (SELBM):** We propose these models, which unify many leading algorithms suited to various data types.

- **Classification Expectation Maximization Approach:** Our proposed algorithms use this approach and have a general framework based on matrix form.

- **Focus on Document-Word Matrices:** While we propose a flexible matrix formalism for different models according to different distributions, we focus on document-word matrices in this work. We evaluate ELBMs and SELBMs using six real document-word matrices and three synthetic datasets.

## Highlights
- Exponential family Latent Block Model (**ELBM**) and Sparse version (**SELBM**) were proposed, which unify many models with various data types.
- The proposed algorithms using the classification expectation maximization approach have a general framework based on matrix form.
- Using six real document-word matrices and three synthetic datasets (Bernoulli, Poisson, Gaussian), we compared **ELBM** with **SELBM**.
- All datasets and algorithm codes are available on GitHub as [**`ELBMcoclust`**]() repository.

## Supplementary materials
- More details about the **Classic3** real-text dataset are available [here](https://github.com/Saeidhoseinipour/ELBMcoclust/tree/main/Datasets#readme).
- For additional visualization, see [here](https://github.com/Saeidhoseinipour/ELBMcoclust/tree/main/Images).
- Orginal paper is available on [Advances in Data Analysis and Classification (ADAC)](https://link.springer.com/article/10.1007/s11634-024-00608-3#rightslink).
  

## Data Availability
The code of algorithms, all datasets, additional visualizations, and materials are available at [**`ELBMcoclust`**](https://github.com/Saeidhoseinipour/ELBMcoclust) repository. Our experiments were performed on a PC (Intel(R), Core(TM) i7-10510U, 2.30 GHz), and all figures were produced in Python using the [Seaborn](https://seaborn.pydata.org/index.html) and [Matplotlib](https://matplotlib.org/) libraries.

## Cite
Please cite the following paper in your publication if you are using [**`ELBMcoclust`**]() in your research:

    
```bibtex
 @article{ELBMcoclust2024, 
    title=           {A Sparse Exponential Family Latent Block Model for Co-clustering}, 
    Journal=         {Advances in Data Analysis and Classification}
    authors=         {Saeid Hoseinipour, Mina Aminghafari, Adel Mohammadpour, Mohamed Nadif},
    pages=           {1-37},
    DOI=             {https://doi.org/10.1007/s11634-024-00608-3}    
    year=            {2024}
} 
```


<!--

##  Presentation video



[![Presentation video for OPNMTF, Text mining, Matrix factorization, Co-clustering, Saeid Hoseinipour](https://github.com/Saeidhoseinipour/NMTFcoclust/blob/master/Doc/Image/OPNMTF_video.png)](https://www.youtube.com/watch?v=LCamkfTYGyM&t=5s)


<p align="center">
  <a href="https://www.youtube.com/watch?v=LCamkfTYGyM&t=5s">
    <img src="https://github.com/Saeidhoseinipour/NMTFcoclust/blob/master/Doc/Image/OPNMTF_video.png" alt="Expoential family, Latent Block Model, Text mining, Matrix factorization, Co-clustering, Saeid Hoseinipour" style="width:60%; transform: perspective(1000px) rotateY(-70deg);">
  </a>
</p>


<p align="center">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
    <path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C14.09 3.81 15.76 3 17.5 3 20.58 3 23 5.42 23 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/>
  </svg>
</p>



<a href="">
  <img src="https://github.com/Saeidhoseinipour/ELBMcoclust/blob/main/Images/Cover_SELBM.png" alt="Presentation video for OPNMTF, Text mining, Matrix factorization, Co-clustering, Saeid Hoseinipour" style="width: 70%;">
</a>

-->


## References
[1] [Ailem, Melissa et al, Sparse Poisson latent block model for document clustering, *IEEE Transactions on Knowledge and Data Engineering* (2017a).](https://ieeexplore.ieee.org/abstract/document/7876732)

[2] [Ailem, Melissa et al, Model-based co-clustering for the effective handling of sparse data, *Pattern Recognition* (2017b)](https://doi.org/10.1016/j.patcog.2017.06.005) 

[3] [Govaert and Nadif, Clustering with block mixture models, *Pattern Recognition* (2013).](https://www.sciencedirect.com/science/article/abs/pii/S0031320302000742)

[4] [Govaert and Nadif, Block clustering with Bernoulli mixture models: Comparison of different approaches, *Computational Statistics and Data Analysis* (2008).](https://www.sciencedirect.com/science/article/abs/pii/S0167947307003441)

[5] [Govaert and Nadif, Latent block model for contingency table, *Communications in Statistics - Theory and Methods* (2010).](https://doi.org/10.1080/03610920903140197)

[6] [Govaert and Nadif, Co-clustering: models, algorithms and applications, *John Wiley and Sons* (2013).](https://doi.org/nmOgI1bz_1XW_hItrgRrzcDzc10)

[7] [Rodolphe Priam et al, Topographic Bernoulli block mixture mapping for binary tables, *Pattern Analysis and Applications* (2014).](https://link.springer.com/article/10.1007/s10044-014-0368-8)

[8] [DasGupta, The exponential family and statistical applications, *Probability for Statistics and Machine Learning: Fundamentals and Advanced Topics* (2011).](https://doi.org/10.1007/978-1-4419-9634-3_18)

[9] [Del Buono et al, Non-negative matrix tri-factorization for co-clustering: an analysis of the block matrix, *Information Sciences* (2015).](https://doi.org/10.1016/j.ins.2014.12.058)

[10] [Laclau et al, Diagonal latent block model for binary data, *Statistics and Computing* (2017).](https://doi.org/10.1007/s11222-016-9677-7)

[11] [Fossier, Riverain et al, Semi-supervised Latent Block Model with pairwise constraints, *Machine Learning* (2022).](https://link.springer.com/article/10.1007/s10994-022-06137-4)

[12]  [Hartigan, Direct clustering of a data matrix, *Journal of the American Statistical Association* (1972)](https://doi.org/10.1080/01621459.1972.1048121)

[13] [Hoseinipour et al, Orthogonal parametric non-negative matrix tri-factorization with $\alpha$-Divergence for co-clustering, *Expert Systems with Applications* (2023).](https://doi.org/10.1016/j.eswa.2023.120680)
